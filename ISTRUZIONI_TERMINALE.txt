================================================================================
PROGETTO BIG DATA 2023/2024 - ISTRUZIONI TERMINALE
================================================================================

Studenti:
  - Francesco Quagliuolo (0622702412) - f.quagliuolo@studenti.unisa.it
  - Giuseppe Alfonso Mangiola (0622702372) - g.mangiola1@studenti.unisa.it
Canale: IZ

================================================================================
AVVIO CLUSTER
================================================================================

# 1. Avviare i container
cd /home/quaily/progetto_bigdata
docker-compose up -d

# 2. Entrare nel master
docker exec -it master bash

# 3. Impostare le variabili d'ambiente (NECESSARIO ogni volta che entri)
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin

# 4. Formattare HDFS (solo la PRIMA VOLTA o dopo reset)
ssh slave1 "rm -rf /tmp/hadoop-*"
ssh slave2 "rm -rf /tmp/hadoop-*"
ssh slave3 "rm -rf /tmp/hadoop-*"
rm -rf /tmp/hadoop-*
hdfs namenode -format

# 5. Avviare HDFS e YARN
start-dfs.sh
start-yarn.sh

# 6. Verificare che tutto funzioni
hdfs dfsadmin -report
# Deve mostrare: Live datanodes (3)


================================================================================
ESERCIZIO 1 - HADOOP MAPREDUCE
================================================================================

cd /data
chmod +x scripts/run_mapreduce.sh
./scripts/run_mapreduce.sh

# Risultati in: /data/output_mapreduce/part-r-00000


================================================================================
ESERCIZIO 2 - APACHE SPARK
================================================================================

cd /data
chmod +x scripts/run_spark_local.sh
./scripts/run_spark_local.sh

# Risultati in: /data/output_spark/part-*


================================================================================
SPEGNIMENTO
================================================================================

# Dal container master:
stop-yarn.sh
stop-dfs.sh
exit

# Dal terminale locale:
cd /home/quaily/progetto_bigdata
docker-compose down


================================================================================
SE RIENTRI DOPO AVER SPENTO (senza format)
================================================================================

cd /home/quaily/progetto_bigdata
docker-compose up -d
docker exec -it master bash

export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin

start-dfs.sh
start-yarn.sh
hdfs dfsadmin -report

# Se i datanodes non si connettono, riformatta (vedi sezione AVVIO CLUSTER)
# NOTA: dopo un format devi ricaricare il dataset su HDFS rieseguendo gli script


================================================================================
